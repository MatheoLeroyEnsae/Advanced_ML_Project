{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585ac353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/python/lib/python3.13/site-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from datasets) (2.4.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/python/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/python/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/python/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/python/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/python/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/python/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/python/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/python/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/python/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/python/lib/python3.13/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/python/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/python/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/python/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /opt/python/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/python/lib/python3.13/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: pyyaml in /opt/python/lib/python3.13/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/python/lib/python3.13/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/python/lib/python3.13/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2026.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/python/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/python/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (2026.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/python/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/python/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/python/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: evaluate in /opt/python/lib/python3.13/site-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/python/lib/python3.13/site-packages (from evaluate) (4.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from evaluate) (2.4.0)\n",
      "Requirement already satisfied: dill in /opt/python/lib/python3.13/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/python/lib/python3.13/site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/python/lib/python3.13/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/python/lib/python3.13/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/python/lib/python3.13/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /opt/python/lib/python3.13/site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/python/lib/python3.13/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/python/lib/python3.13/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/python/lib/python3.13/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/python/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/python/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/python/lib/python3.13/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/python/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/lib/python3.13/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/lib/python3.13/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.13/site-packages (from pandas->evaluate) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install accelerate\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db8de2",
   "metadata": {},
   "source": [
    "# Funcction s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fe912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import datasets\n",
    "import random \n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ace869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(dataset_name, seed, add_options=None):\n",
    "    \"\"\"Load dataset.\"\"\"\n",
    "\n",
    "    train_dataset, validation_dataset = None, None\n",
    "\n",
    "    if dataset_name == \"trivia_qa\":\n",
    "            dataset = datasets.load_dataset('TimoImhof/TriviaQA-in-SQuAD-format')['unmodified']\n",
    "            dataset = dataset.train_test_split(test_size=0.2, seed=seed)\n",
    "            train_dataset = dataset['train']\n",
    "            validation_dataset = dataset['test']\n",
    "    return train_dataset, validation_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36063cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = load_ds( \"trivia_qa\", 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a407c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    \"\"\"Get indices of answerable and unanswerable questions.\"\"\"\n",
    "\n",
    "    def clen(ex):\n",
    "        return len(ex[\"answers\"][\"text\"])\n",
    "\n",
    "    answerable_indices = [i for i, ex in enumerate(dataset) if clen(ex) > 0]\n",
    "    unanswerable_indices = [i for i, ex in enumerate(dataset) if clen(ex) == 0]\n",
    "\n",
    "    # union == full dataset\n",
    "    assert set(answerable_indices) | set(\n",
    "        unanswerable_indices) == set(range(len(dataset)))\n",
    "    # no overlap\n",
    "    assert set(answerable_indices) - \\\n",
    "        set(unanswerable_indices) == set(answerable_indices)\n",
    "\n",
    "    return answerable_indices, unanswerable_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930c9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_indices, unanswerable_indices = split_dataset(train_dataset)\n",
    "\n",
    "unanswerable_indices = []\n",
    "val_answerable, val_unanswerable = split_dataset(validation_dataset)\n",
    "del val_unanswerable\n",
    "validation_dataset = [validation_dataset[i] for i in val_answerable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ee695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_few_shot = 3\n",
    "prompt_indices = random.sample(answerable_indices, num_few_shot)\n",
    "remaining_answerable = list(set(answerable_indices) - set(prompt_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f2b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_make_prompt():\n",
    "  def make_prompt(context, question, answer, brief, brief_always):\n",
    "            prompt = ''\n",
    "            if brief_always:\n",
    "                prompt += brief\n",
    "            if (context is not None):\n",
    "                prompt += f\"Context: {context}\\n\"\n",
    "            prompt += f\"Question: {question}\\n\"\n",
    "            if answer:\n",
    "                prompt += f\"Answer: {answer}\\n\\n\"\n",
    "            else:\n",
    "                prompt += 'Answer:'\n",
    "            return prompt\n",
    "        \n",
    "  return make_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2bbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_fewshot_prompt_from_indices(dataset, example_indices, brief, brief_always, make_prompt):\n",
    "    \"\"\"Given a dataset and indices, construct a fewshot prompt.\"\"\"\n",
    "    if not brief_always:\n",
    "        prompt = brief\n",
    "    else:\n",
    "        prompt = ''\n",
    "\n",
    "    for example_index in example_indices:\n",
    "\n",
    "        example = dataset[example_index]\n",
    "        context = example[\"context\"]\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "        prompt = prompt + make_prompt(context, question, answer, brief, brief_always)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bbe0d",
   "metadata": {},
   "source": [
    "# HugginFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fcd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Text\n",
    "\n",
    "\n",
    "STOP_SEQUENCES = ['\\n\\n\\n\\n', '\\n\\n\\n', '\\n\\n', '\\n', 'Question:', 'Context:']\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "\n",
    "    stop_sequences: List[Text]\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, input_data, temperature):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_p_true(self, input_data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7817ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement HuggingfaceModel models.\"\"\"\n",
    "import copy\n",
    "import logging\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import StoppingCriteria\n",
    "from transformers import StoppingCriteriaList\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    \"\"\"Stop generations when they match a particular text or token.\"\"\"\n",
    "    def __init__(self, stops, tokenizer, match_on='text', initial_length=None):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.initial_length = initial_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.match_on = match_on\n",
    "        if self.match_on == 'tokens':\n",
    "            self.stops = [torch.tensor(self.tokenizer.encode(i)).to('cuda') for i in self.stops]\n",
    "            print(self.stops)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        del scores  # `scores` arg is required by StoppingCriteria but unused by us.\n",
    "        for stop in self.stops:\n",
    "            if self.match_on == 'text':\n",
    "                generation = self.tokenizer.decode(input_ids[0][self.initial_length:], skip_special_tokens=False)\n",
    "                match = stop in generation\n",
    "            elif self.match_on == 'tokens':\n",
    "                # Can be dangerous due to tokenizer ambiguities.\n",
    "                match = stop in input_ids[0][-len(stop):]\n",
    "            else:\n",
    "                raise\n",
    "            if match:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def remove_split_layer(device_map_in):\n",
    "    \"\"\"Modify device maps s.t. individual layers are not spread across devices.\"\"\"\n",
    "\n",
    "    device_map = copy.deepcopy(device_map_in)\n",
    "    destinations = list(device_map.keys())\n",
    "\n",
    "    counts = Counter(['.'.join(i.split('.')[:2]) for i in destinations])\n",
    "\n",
    "    found_split = False\n",
    "    for layer, count in counts.items():\n",
    "        if count == 1:\n",
    "            continue\n",
    "\n",
    "        if found_split:\n",
    "            # Only triggers if we find more than one split layer.\n",
    "            raise ValueError(\n",
    "                'More than one split layer.\\n'\n",
    "                f'Currently at layer {layer}.\\n'\n",
    "                f'In map: {device_map_in}\\n'\n",
    "                f'Out map: {device_map}\\n')\n",
    "\n",
    "        logging.info(f'Split layer is {layer}.')\n",
    "\n",
    "        # Remove split for that layer.\n",
    "        for name in list(device_map.keys()):\n",
    "            if name.startswith(layer):\n",
    "                print(f'pop {name}')\n",
    "                device = device_map.pop(name)\n",
    "\n",
    "        device_map[layer] = device\n",
    "        found_split = True\n",
    "\n",
    "    return device_map\n",
    "\n",
    "\n",
    "class HuggingfaceModel(BaseModel):\n",
    "    \"\"\"Hugging Face Model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name, stop_sequences=None, max_new_tokens=None):\n",
    "        if max_new_tokens is None:\n",
    "            raise\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        if stop_sequences == 'default':\n",
    "            stop_sequences = STOP_SEQUENCES\n",
    "\n",
    "        if 'llama' in model_name.lower():\n",
    "            if model_name.endswith('-8bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,)}\n",
    "                model_name = model_name[:-len('-8bit')]\n",
    "                eightbit = True\n",
    "            else:\n",
    "                kwargs = {}\n",
    "                eightbit = False\n",
    "\n",
    "            if 'Llama-2' in model_name or \"Llama-3\" in model_name:\n",
    "                base = 'meta-llama'\n",
    "                if 'Llama-2' in model_name:\n",
    "                    model_name = model_name + '-hf'\n",
    "            else:\n",
    "                base = 'huggyllama'\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                f\"{base}/{model_name}\", device_map=\"auto\",\n",
    "                token_type_ids=None)\n",
    "\n",
    "            llama65b = '65b' in model_name and base == 'huggyllama'\n",
    "            llama2_70b = '70b' in model_name and base == 'meta-llama'\n",
    "\n",
    "            if ('7b' in model_name or \"8B\" in model_name or '13b' in model_name) or eightbit:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    f\"{base}/{model_name}\", device_map=\"auto\",\n",
    "                    max_memory={0: '80GIB'}, **kwargs,)\n",
    "\n",
    "            elif llama2_70b or llama65b:\n",
    "                path = snapshot_download(\n",
    "                    repo_id=f'{base}/{model_name}',\n",
    "                    allow_patterns=['*.json', '*.model', '*.safetensors'],\n",
    "                    ignore_patterns=['pytorch_model.bin.index.json']\n",
    "                )\n",
    "\n",
    "                config = AutoConfig.from_pretrained(f\"{base}/{model_name}\")\n",
    "                with accelerate.init_empty_weights():\n",
    "                    self.model = AutoModelForCausalLM.from_config(config)\n",
    "                self.model.tie_weights()\n",
    "                max_mem = 15 * 4686198491\n",
    "\n",
    "                device_map = accelerate.infer_auto_device_map(\n",
    "                    self.model.model,\n",
    "                    max_memory={0: max_mem, 1: max_mem},\n",
    "                    dtype='float16'\n",
    "                )\n",
    "                device_map = remove_split_layer(device_map)\n",
    "                full_model_device_map = {f\"model.{k}\": v for k, v in device_map.items()}\n",
    "                full_model_device_map[\"lm_head\"] = 0\n",
    "\n",
    "                self.model = accelerate.load_checkpoint_and_dispatch(\n",
    "                    self.model, path, device_map=full_model_device_map,\n",
    "                    dtype='float16', skip_keys='past_key_values')\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "        elif 'mistral' in model_name.lower():\n",
    "\n",
    "            if model_name.endswith('-8bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,)}\n",
    "                model_name = model_name[:-len('-8bit')]\n",
    "            if model_name.endswith('-4bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,)}\n",
    "                model_name = model_name[:-len('-4bit')]\n",
    "            else:\n",
    "                kwargs = {}\n",
    "\n",
    "            model_id = f'mistralai/{model_name}'\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id, device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                device_map='auto',\n",
    "                max_memory={0: '80GIB'},\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        elif 'falcon' in model_name:\n",
    "            \n",
    "\n",
    "            model_id = f'tiiuae/{model_name}'\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id, device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "            )\n",
    "            \n",
    "        elif 'Falcon' in model_name:\n",
    "            model_id = f'tiiuae/{model_name}'\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id, device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "            )\n",
    "        \n",
    "        elif \"Qwen\" in model_name:\n",
    "\n",
    "            model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_id,device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.stop_sequences = stop_sequences + [self.tokenizer.eos_token]\n",
    "        self.token_limit = 4096 if 'Llama-2' in model_name or \"Llama-3\" in model_name else 2048\n",
    "\n",
    "    def predict(self, input_data, temperature, return_full=False):\n",
    "\n",
    "        # Implement prediction.\n",
    "        inputs = self.tokenizer(input_data, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        if 'llama' in self.model_name.lower() or 'falcon' in self.model_name or 'mistral' in self.model_name.lower():\n",
    "            if 'token_type_ids' in inputs:  # Some HF models have changed.\n",
    "                del inputs['token_type_ids']\n",
    "            pad_token_id = self.tokenizer.eos_token_id\n",
    "        else:\n",
    "            pad_token_id = None\n",
    "\n",
    "        if self.stop_sequences is not None:\n",
    "            stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(\n",
    "                stops=self.stop_sequences,\n",
    "                initial_length=len(inputs['input_ids'][0]),\n",
    "                tokenizer=self.tokenizer)])\n",
    "        else:\n",
    "            stopping_criteria = None\n",
    "\n",
    "        logging.debug('temperature: %f', temperature)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                output_hidden_states=True,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "            )\n",
    "\n",
    "        if len(outputs.sequences[0]) > self.token_limit:\n",
    "            raise ValueError(\n",
    "                'Generation exceeding token limit %d > %d',\n",
    "                len(outputs.sequences[0]), self.token_limit)\n",
    "\n",
    "        full_answer = self.tokenizer.decode(\n",
    "            outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "        if return_full:\n",
    "            return full_answer\n",
    "\n",
    "        # For some models, we need to remove the input_data from the answer.\n",
    "        if full_answer.startswith(input_data):\n",
    "            input_data_offset = len(input_data)\n",
    "        else:\n",
    "            #raise ValueError('Have not tested this in a while.')\n",
    "            logging.error(f\"Full answer should start from input_data. Setting input_data offset to 0\")\n",
    "            logging.error(f\"Full answer is {full_answer}\")\n",
    "            logging.error(f\"Input data is {input_data}\")\n",
    "            input_data_offset = 0\n",
    "\n",
    "        # Remove input from answer.\n",
    "        answer = full_answer[input_data_offset:]\n",
    "\n",
    "        # Remove stop_words from answer.\n",
    "        stop_at = len(answer)\n",
    "        sliced_answer = answer\n",
    "        if self.stop_sequences is not None:\n",
    "            for stop in self.stop_sequences:\n",
    "                if answer.endswith(stop):\n",
    "                    stop_at = len(answer) - len(stop)\n",
    "                    sliced_answer = answer[:stop_at]\n",
    "                    break\n",
    "            if not all([stop not in sliced_answer for stop in self.stop_sequences]):\n",
    "                error_msg = 'Error: Stop words not removed successfully!'\n",
    "                error_msg += f'Answer: >{answer}< '\n",
    "                error_msg += f'Sliced Answer: >{sliced_answer}<'\n",
    "                if 'falcon' not in self.model_name.lower():\n",
    "                    raise ValueError(error_msg)\n",
    "                else:\n",
    "                    logging.error(error_msg)\n",
    "\n",
    "        # Remove whitespaces from answer (in particular from beginning.)\n",
    "        sliced_answer = sliced_answer.strip()\n",
    "\n",
    "        # Get the number of tokens until the stop word comes up.\n",
    "        # Note: Indexing with `stop_at` already excludes the stop_token.\n",
    "        # Note: It's important we do this with full answer, since there might be\n",
    "        # non-trivial interactions between the input_data and generated part\n",
    "        # in tokenization (particularly around whitespaces.)\n",
    "        token_stop_index = self.tokenizer(full_answer[:input_data_offset + stop_at], return_tensors=\"pt\")['input_ids'].shape[1]\n",
    "        n_input_token = len(inputs['input_ids'][0])\n",
    "        n_generated = token_stop_index - n_input_token\n",
    "\n",
    "        if n_generated == 0:\n",
    "            logging.warning('Only stop_words were generated. For likelihoods and embeddings, taking stop word instead.')\n",
    "            n_generated = 1\n",
    "\n",
    "        # Get the last hidden state (last layer) and the last token's embedding of the answer.\n",
    "        # Note: We do not want this to be the stop token.\n",
    "\n",
    "        # outputs.hidden_state is a tuple of len = n_generated_tokens.\n",
    "        # The first hidden state is for the input tokens and is of shape\n",
    "        #     (n_layers) x (batch_size, input_size, hidden_size).\n",
    "        # (Note this includes the first generated token!)\n",
    "        # The remaining hidden states are for the remaining generated tokens and is of shape\n",
    "        #    (n_layers) x (batch_size, 1, hidden_size).\n",
    "\n",
    "        # Note: The output embeddings have the shape (batch_size, generated_length, hidden_size).\n",
    "        # We do not get embeddings for input_data! We thus subtract the n_tokens_in_input from\n",
    "        # token_stop_index to arrive at the right output.\n",
    "\n",
    "        if 'decoder_hidden_states' in outputs.keys():\n",
    "            hidden = outputs.decoder_hidden_states\n",
    "        else:\n",
    "            hidden = outputs.hidden_states\n",
    "\n",
    "        if len(hidden) == 1:\n",
    "            logging.warning(\n",
    "                'Taking first and only generation for hidden! '\n",
    "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
    "                'last_token: %s, generation was: %s',\n",
    "                n_generated, n_input_token, token_stop_index,\n",
    "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
    "                full_answer,\n",
    "                )\n",
    "            last_input = hidden[0]\n",
    "        elif ((n_generated - 1) >= len(hidden)):\n",
    "            # If access idx is larger/equal.\n",
    "            logging.error(\n",
    "                'Taking last state because n_generated is too large'\n",
    "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
    "                'last_token: %s, generation was: %s, slice_answer: %s',\n",
    "                n_generated, n_input_token, token_stop_index,\n",
    "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
    "                full_answer, sliced_answer\n",
    "                )\n",
    "            last_input = hidden[-1]\n",
    "        else:\n",
    "            last_input = hidden[n_generated - 1]\n",
    "\n",
    "        # Then access last layer for input\n",
    "        last_layer = last_input[-1]\n",
    "        # Then access last token in input.\n",
    "        last_token_embedding = last_layer[:, -1, :].cpu()\n",
    "\n",
    "        # Get log_likelihoods.\n",
    "        # outputs.scores are the logits for the generated token.\n",
    "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
    "        # Each entry is shape (bs, vocabulary size).\n",
    "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
    "        transition_scores = self.model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
    "\n",
    "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
    "        if len(log_likelihoods) == 1:\n",
    "            logging.warning('Taking first and only generation for log likelihood!')\n",
    "            log_likelihoods = log_likelihoods\n",
    "        else:\n",
    "            log_likelihoods = log_likelihoods[:n_generated]\n",
    "\n",
    "        if len(log_likelihoods) == self.max_new_tokens:\n",
    "            logging.warning('Generation interrupted by max_token limit.')\n",
    "\n",
    "        if len(log_likelihoods) == 0:\n",
    "            raise ValueError\n",
    "\n",
    "        return sliced_answer, log_likelihoods, last_token_embedding\n",
    "\n",
    "    def get_p_true(self, input_data):\n",
    "        \"\"\"Get the probability of the model anwering A (True) for the given input.\"\"\"\n",
    "\n",
    "        input_data += ' A'\n",
    "        tokenized_prompt_true = self.tokenizer(input_data, return_tensors='pt').to('cuda')['input_ids']\n",
    "        # The computation of the negative log likelihoods follows:\n",
    "        # https://huggingface.co/docs/transformers/perplexity.\n",
    "\n",
    "        target_ids_true = tokenized_prompt_true.clone()\n",
    "        # Set all target_ids except the last one to -100.\n",
    "        target_ids_true[0, :-1] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output_true = self.model(tokenized_prompt_true, labels=target_ids_true)\n",
    "\n",
    "        loss_true = model_output_true.loss\n",
    "\n",
    "        return -loss_true.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4442d94",
   "metadata": {},
   "source": [
    "# suite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad6753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIEF_PROMPTS = {\n",
    "    'default': \"Answer the following question as briefly as possible.\\n\",\n",
    "    'chat': 'Answer the following question in a single brief but complete sentence.\\n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c3bff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prompt = get_make_prompt()\n",
    "BRIEF = BRIEF_PROMPTS['default']\n",
    "prompt = construct_fewshot_prompt_from_indices(\n",
    "train_dataset, prompt_indices, BRIEF, False, make_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96642110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model_name, model_max_new_tokens):\n",
    "    if 'Qwen' in model_name or 'falcon' in model_name or 'mistral' in model_name.lower():\n",
    "        model = HuggingfaceModel(\n",
    "            model_name, stop_sequences='default',\n",
    "            max_new_tokens=model_max_new_tokens)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model_name `{model_name}`.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f517e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"falcon-7b-instruct\"\n",
    "model_name = \"Qwen2.5-1.5B-Instruct\"\n",
    "model_max_new_tokens = 100\n",
    "model = init_model(model_name, model_max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf0bb744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU libre  (GB): 11.492919921875\n",
      "GPU total  (GB): 14.56805419921875\n"
     ]
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "print(\"GPU libre  (GB):\", free / 1024**3)\n",
    "print(\"GPU total  (GB):\", total / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca893ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true_num_fewshot = 4 \n",
    "num_generations = 5\n",
    "metric = 'squad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(metric):\n",
    "    if metric == 'squad':\n",
    "\n",
    "        squad_metric = load(\"squad_v2\")\n",
    "\n",
    "        def metric_fct(response, example, *args, **kwargs):\n",
    "            # Compatibility with recomputation.\n",
    "            if 'id' in example:\n",
    "                exid = example['id']\n",
    "            elif 'id' in example['reference']:\n",
    "                exid = example['reference']['id']\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            prediction = {'prediction_text': response, 'no_answer_probability': 0.0, 'id': exid}\n",
    "            results = squad_metric.compute(\n",
    "                predictions=[prediction],\n",
    "                references=[get_reference(example)])\n",
    "            return 1.0 if (results['f1'] >= 50.0) else 0.0\n",
    "    return metric_fct\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "686b5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.47kB [00:00, 7.43MB/s]\n",
      "Downloading extra modules: 11.3kB [00:00, 15.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "metric = get_metric(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1ab22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute p_true uncertainty metric.\"\"\"\n",
    "import logging\n",
    "\n",
    "\n",
    "def construct_few_shot_prompt(\n",
    "        *, model, dataset, indices, prompt, brief, brief_always, make_prompt,\n",
    "        num_generations, metric):\n",
    "    \"\"\"Construct few shot prompt for p_true uncertainty metric.\"\"\"\n",
    "\n",
    "    # Call model n_shots many times.\n",
    "    few_shot_prompt = []\n",
    "    all_responses = dict()\n",
    "    for it, i in enumerate(indices):\n",
    "        prompt_candidate = []\n",
    "        example = dataset[i]\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"context\"]\n",
    "        if it != 0:\n",
    "            prompt_candidate += ['\\n']\n",
    "        prompt_candidate += ['Question: ' + question]\n",
    "        prompt_candidate += ['\\nBrainstormed Answers: ']\n",
    "        current_question = make_prompt(context, question, None, brief, brief_always)\n",
    "        local_prompt = prompt + current_question\n",
    "        logging.info('P_TRUE >> Current Question: '.ljust(25) + current_question)\n",
    "\n",
    "        responses = []\n",
    "        for j in range(num_generations + 1):\n",
    "\n",
    "            if j == 0:\n",
    "                temperature = 0.1\n",
    "            else:\n",
    "                temperature = 1.0\n",
    "\n",
    "            response, _, _ = model.predict(local_prompt, temperature)\n",
    "            logging.info('P_TRUE >> Current Response: '.ljust(25) + response)\n",
    "\n",
    "            responses.append(response)\n",
    "            prompt_candidate += [f'{response.strip()} \\n']\n",
    "            if j == 0:\n",
    "                # Save most likely response and compute correctness metric for it.\n",
    "                most_likely_response = response\n",
    "                is_correct = metric(response, example, model)\n",
    "                answers = [answer for answer in example['answers']['text']]\n",
    "                logging.info('P_TRUE >> LOW-T >> true answer: '.ljust(35) + str(answers))\n",
    "                logging.info('P_TRUE >> LOW-T >> acc: '.ljust(35) + str(is_correct))\n",
    "\n",
    "        all_responses[i] = dict(\n",
    "            responses=responses, most_likely_response=most_likely_response,\n",
    "            is_correct=is_correct)\n",
    "\n",
    "        prompt_candidate += ['Possible answer: ' + most_likely_response + '\\n']\n",
    "        prompt_candidate += ['Is the possible answer:\\n']\n",
    "        prompt_candidate += ['A) True\\n']\n",
    "        prompt_candidate += ['B) False\\n']\n",
    "        prompt_candidate += ['The possible answer is:']\n",
    "        prompt_candidate += [' A' if is_correct else ' B']\n",
    "\n",
    "        prompt_len = len(model.tokenizer.encode(''.join(few_shot_prompt + prompt_candidate)))\n",
    "        # At test time, get a maximum of `num_generations * model.token_limit` extra tokens\n",
    "        # 200 buffer for question and 'Possible Answer'.\n",
    "        max_input_len = prompt_len + num_generations * model.max_new_tokens + 200\n",
    "\n",
    "        if max_input_len < model.token_limit:\n",
    "            few_shot_prompt.extend(prompt_candidate)\n",
    "        else:\n",
    "            logging.warning('Cutting of p_true prompt at length %d.', it)\n",
    "            break\n",
    "\n",
    "    return ''.join(few_shot_prompt), all_responses, it\n",
    "\n",
    "\n",
    "def calculate_p_true(\n",
    "        model, question, most_probable_answer, brainstormed_answers,\n",
    "        few_shot_prompt, hint=False):\n",
    "    \"\"\"Calculate p_true uncertainty metric.\"\"\"\n",
    "\n",
    "    if few_shot_prompt:\n",
    "        prompt = few_shot_prompt + '\\n'\n",
    "    else:\n",
    "        prompt = ''\n",
    "\n",
    "    prompt += 'Question: ' + question\n",
    "    prompt += '\\nBrainstormed Answers: '\n",
    "    for answer in brainstormed_answers + [most_probable_answer]:\n",
    "        prompt += answer.strip() + '\\n'\n",
    "    prompt += 'Possible answer: ' + most_probable_answer + '\\n'\n",
    "    if not hint:\n",
    "        prompt += 'Is the possible answer:\\n'\n",
    "        prompt += 'A) True\\n'\n",
    "        prompt += 'B) False\\n'\n",
    "        prompt += 'The possible answer is:'\n",
    "    else:\n",
    "        prompt += 'Do the brainstormed answers match the possible answer? Respond with A if they do, if they do not respond with B. Answer:'\n",
    "\n",
    "    log_prob = model.get_p_true(prompt)\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "281ed880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference(example):\n",
    "    if 'answers' not in example:\n",
    "        example = example['reference']\n",
    "    answers = example['answers']\n",
    "    answer_starts = answers.get('answer_start', [])\n",
    "    reference = {'answers': {'answer_start': answer_starts, 'text': answers['text']}, 'id': example['id']}\n",
    "    return reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64629b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "p_true_indices = random.sample(answerable_indices, p_true_num_fewshot)\n",
    "remaining_answerable = list(set(remaining_answerable) - set(p_true_indices))\n",
    "p_true_few_shot_prompt, p_true_responses, len_p_true = construct_few_shot_prompt(\n",
    "            model=model, dataset=train_dataset, indices=p_true_indices,\n",
    "            prompt=prompt, brief=BRIEF,\n",
    "            brief_always=False,\n",
    "            make_prompt=make_prompt, num_generations=num_generations,\n",
    "            metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c122681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question: Which river flows through Cork City where it splits in two for a short distance, creating an island on which Cork's city centre is built?\\nBrainstormed Answers: River Lee \\nRiver Lee \\nRiver Lee \\nRiver Lee \\nRiver Lee \\nRiver Lee \\nPossible answer: River Lee\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A\\nQuestion: In music how many crotchets make up a semibreve?\\nBrainstormed Answers: 4 \\n4 \\ntwo \\n4 \\n2 \\n4 \\nPossible answer: 4\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: B\\nQuestion: Kelly Jones, Richard Jones and Richard Cable founded which group\\nBrainstormed Answers: Stereophonics \\nStereophonics \\nStereophonics \\nStereophonics \\nStereophonics \\nStereophonics \\nPossible answer: Stereophonics\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A\\nQuestion: What was the previous name of the city of Maputo?\\nBrainstormed Answers: none given \\nBeira \\nNone. According to the context provided, Maputo does not have a former name. \\nDar es Salaam \\nNone, Maputo is always known as such. \\nNot mentioned in context. \\nPossible answer: none given\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: B\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_true_few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "936d5518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9648: {'responses': ['River Lee',\n",
       "   'River Lee',\n",
       "   'River Lee',\n",
       "   'River Lee',\n",
       "   'River Lee',\n",
       "   'River Lee'],\n",
       "  'most_likely_response': 'River Lee',\n",
       "  'is_correct': 1.0},\n",
       " 5663: {'responses': ['4', '4', 'two', '4', '2', '4'],\n",
       "  'most_likely_response': '4',\n",
       "  'is_correct': 0.0},\n",
       " 1953: {'responses': ['Stereophonics',\n",
       "   'Stereophonics',\n",
       "   'Stereophonics',\n",
       "   'Stereophonics',\n",
       "   'Stereophonics',\n",
       "   'Stereophonics'],\n",
       "  'most_likely_response': 'Stereophonics',\n",
       "  'is_correct': 1.0},\n",
       " 1977: {'responses': ['none given',\n",
       "   'Beira',\n",
       "   'None. According to the context provided, Maputo does not have a former name.',\n",
       "   'Dar es Salaam',\n",
       "   'None, Maputo is always known as such.',\n",
       "   'Not mentioned in context.'],\n",
       "  'most_likely_response': 'none given',\n",
       "  'is_correct': 0.0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_true_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a2505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb2f82d8",
   "metadata": {},
   "source": [
    "# Annexe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb; print(bnb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59921ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; print(torch.version.cuda, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0  # optionnel, peut stabiliser la mmoire\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",       # rpartit sur GPU(s)\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970662fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e362a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78197668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ff7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU libre  (GB): 11.492919921875\n",
      "GPU total  (GB): 14.56805419921875\n"
     ]
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "print(\"GPU libre  (GB):\", free / 1024**3)\n",
    "print(\"GPU total  (GB):\", total / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b2914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10016be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6efe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible :\", torch.cuda.is_available())\n",
    "print(\"Nombre de GPU :\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    idx = torch.cuda.current_device()\n",
    "    print(\"GPU actuel :\", torch.cuda.get_device_name(idx))\n",
    "    print(\"VRAM totale (GB) :\", torch.cuda.get_device_properties(idx).total_memory / 1024**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall bitsandbytes -y\n",
    "%pip install bitsandbytes-cuda128\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
