{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ac353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/python/lib/python3.13/site-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from datasets) (2.4.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/python/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/python/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/python/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/python/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/python/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/python/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/python/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/python/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/python/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/python/lib/python3.13/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/python/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/python/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/python/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install accelerate\n",
    "%pip install transformers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34fe912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import datasets\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5ace869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(dataset_name, seed, add_options=None):\n",
    "    \"\"\"Load dataset.\"\"\"\n",
    "\n",
    "    train_dataset, validation_dataset = None, None\n",
    "\n",
    "    if dataset_name == \"trivia_qa\":\n",
    "            dataset = datasets.load_dataset('TimoImhof/TriviaQA-in-SQuAD-format')['unmodified']\n",
    "            dataset = dataset.train_test_split(test_size=0.2, seed=seed)\n",
    "            train_dataset = dataset['train']\n",
    "            validation_dataset = dataset['test']\n",
    "    return train_dataset, validation_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36063cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = load_ds( \"trivia_qa\", 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a407c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    \"\"\"Get indices of answerable and unanswerable questions.\"\"\"\n",
    "\n",
    "    def clen(ex):\n",
    "        return len(ex[\"answers\"][\"text\"])\n",
    "\n",
    "    answerable_indices = [i for i, ex in enumerate(dataset) if clen(ex) > 0]\n",
    "    unanswerable_indices = [i for i, ex in enumerate(dataset) if clen(ex) == 0]\n",
    "\n",
    "    # union == full dataset\n",
    "    assert set(answerable_indices) | set(\n",
    "        unanswerable_indices) == set(range(len(dataset)))\n",
    "    # no overlap\n",
    "    assert set(answerable_indices) - \\\n",
    "        set(unanswerable_indices) == set(answerable_indices)\n",
    "\n",
    "    return answerable_indices, unanswerable_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "930c9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_indices, unanswerable_indices = split_dataset(train_dataset)\n",
    "\n",
    "unanswerable_indices = []\n",
    "val_answerable, val_unanswerable = split_dataset(validation_dataset)\n",
    "del val_unanswerable\n",
    "validation_dataset = [validation_dataset[i] for i in val_answerable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20ee695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_few_shot = 3\n",
    "prompt_indices = random.sample(answerable_indices, num_few_shot)\n",
    "remaining_answerable = list(set(answerable_indices) - set(prompt_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1f2b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_make_prompt():\n",
    "  def make_prompt(context, question, answer, brief, brief_always):\n",
    "            prompt = ''\n",
    "            if brief_always:\n",
    "                prompt += brief\n",
    "            if (context is not None):\n",
    "                prompt += f\"Context: {context}\\n\"\n",
    "            prompt += f\"Question: {question}\\n\"\n",
    "            if answer:\n",
    "                prompt += f\"Answer: {answer}\\n\\n\"\n",
    "            else:\n",
    "                prompt += 'Answer:'\n",
    "            return prompt\n",
    "        \n",
    "  return make_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a2bbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_fewshot_prompt_from_indices(dataset, example_indices, brief, brief_always, make_prompt):\n",
    "    \"\"\"Given a dataset and indices, construct a fewshot prompt.\"\"\"\n",
    "    if not brief_always:\n",
    "        prompt = brief\n",
    "    else:\n",
    "        prompt = ''\n",
    "\n",
    "    for example_index in example_indices:\n",
    "\n",
    "        example = dataset[example_index]\n",
    "        context = example[\"context\"]\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "        prompt = prompt + make_prompt(context, question, answer, brief, brief_always)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bbe0d",
   "metadata": {},
   "source": [
    "# HugginFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1fcd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Text\n",
    "\n",
    "\n",
    "STOP_SEQUENCES = ['\\n\\n\\n\\n', '\\n\\n\\n', '\\n\\n', '\\n', 'Question:', 'Context:']\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "\n",
    "    stop_sequences: List[Text]\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, input_data, temperature):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_p_true(self, input_data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7817ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement HuggingfaceModel models.\"\"\"\n",
    "import copy\n",
    "import logging\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import StoppingCriteria\n",
    "from transformers import StoppingCriteriaList\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    \"\"\"Stop generations when they match a particular text or token.\"\"\"\n",
    "    def __init__(self, stops, tokenizer, match_on='text', initial_length=None):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.initial_length = initial_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.match_on = match_on\n",
    "        if self.match_on == 'tokens':\n",
    "            self.stops = [torch.tensor(self.tokenizer.encode(i)).to('cuda') for i in self.stops]\n",
    "            print(self.stops)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        del scores  # `scores` arg is required by StoppingCriteria but unused by us.\n",
    "        for stop in self.stops:\n",
    "            if self.match_on == 'text':\n",
    "                generation = self.tokenizer.decode(input_ids[0][self.initial_length:], skip_special_tokens=False)\n",
    "                match = stop in generation\n",
    "            elif self.match_on == 'tokens':\n",
    "                # Can be dangerous due to tokenizer ambiguities.\n",
    "                match = stop in input_ids[0][-len(stop):]\n",
    "            else:\n",
    "                raise\n",
    "            if match:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def remove_split_layer(device_map_in):\n",
    "    \"\"\"Modify device maps s.t. individual layers are not spread across devices.\"\"\"\n",
    "\n",
    "    device_map = copy.deepcopy(device_map_in)\n",
    "    destinations = list(device_map.keys())\n",
    "\n",
    "    counts = Counter(['.'.join(i.split('.')[:2]) for i in destinations])\n",
    "\n",
    "    found_split = False\n",
    "    for layer, count in counts.items():\n",
    "        if count == 1:\n",
    "            continue\n",
    "\n",
    "        if found_split:\n",
    "            # Only triggers if we find more than one split layer.\n",
    "            raise ValueError(\n",
    "                'More than one split layer.\\n'\n",
    "                f'Currently at layer {layer}.\\n'\n",
    "                f'In map: {device_map_in}\\n'\n",
    "                f'Out map: {device_map}\\n')\n",
    "\n",
    "        logging.info(f'Split layer is {layer}.')\n",
    "\n",
    "        # Remove split for that layer.\n",
    "        for name in list(device_map.keys()):\n",
    "            if name.startswith(layer):\n",
    "                print(f'pop {name}')\n",
    "                device = device_map.pop(name)\n",
    "\n",
    "        device_map[layer] = device\n",
    "        found_split = True\n",
    "\n",
    "    return device_map\n",
    "\n",
    "\n",
    "class HuggingfaceModel(BaseModel):\n",
    "    \"\"\"Hugging Face Model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name, stop_sequences=None, max_new_tokens=None):\n",
    "        if max_new_tokens is None:\n",
    "            raise\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        if stop_sequences == 'default':\n",
    "            stop_sequences = STOP_SEQUENCES\n",
    "\n",
    "        if 'llama' in model_name.lower():\n",
    "            if model_name.endswith('-8bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,)}\n",
    "                model_name = model_name[:-len('-8bit')]\n",
    "                eightbit = True\n",
    "            else:\n",
    "                kwargs = {}\n",
    "                eightbit = False\n",
    "\n",
    "            if 'Llama-2' in model_name or \"Llama-3\" in model_name:\n",
    "                base = 'meta-llama'\n",
    "                if 'Llama-2' in model_name:\n",
    "                    model_name = model_name + '-hf'\n",
    "            else:\n",
    "                base = 'huggyllama'\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                f\"{base}/{model_name}\", device_map=\"auto\",\n",
    "                token_type_ids=None)\n",
    "\n",
    "            llama65b = '65b' in model_name and base == 'huggyllama'\n",
    "            llama2_70b = '70b' in model_name and base == 'meta-llama'\n",
    "\n",
    "            if ('7b' in model_name or \"8B\" in model_name or '13b' in model_name) or eightbit:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    f\"{base}/{model_name}\", device_map=\"auto\",\n",
    "                    max_memory={0: '80GIB'}, **kwargs,)\n",
    "\n",
    "            elif llama2_70b or llama65b:\n",
    "                path = snapshot_download(\n",
    "                    repo_id=f'{base}/{model_name}',\n",
    "                    allow_patterns=['*.json', '*.model', '*.safetensors'],\n",
    "                    ignore_patterns=['pytorch_model.bin.index.json']\n",
    "                )\n",
    "\n",
    "                config = AutoConfig.from_pretrained(f\"{base}/{model_name}\")\n",
    "                with accelerate.init_empty_weights():\n",
    "                    self.model = AutoModelForCausalLM.from_config(config)\n",
    "                self.model.tie_weights()\n",
    "                max_mem = 15 * 4686198491\n",
    "\n",
    "                device_map = accelerate.infer_auto_device_map(\n",
    "                    self.model.model,\n",
    "                    max_memory={0: max_mem, 1: max_mem},\n",
    "                    dtype='float16'\n",
    "                )\n",
    "                device_map = remove_split_layer(device_map)\n",
    "                full_model_device_map = {f\"model.{k}\": v for k, v in device_map.items()}\n",
    "                full_model_device_map[\"lm_head\"] = 0\n",
    "\n",
    "                self.model = accelerate.load_checkpoint_and_dispatch(\n",
    "                    self.model, path, device_map=full_model_device_map,\n",
    "                    dtype='float16', skip_keys='past_key_values')\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "        elif 'mistral' in model_name.lower():\n",
    "\n",
    "            if model_name.endswith('-8bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,)}\n",
    "                model_name = model_name[:-len('-8bit')]\n",
    "            if model_name.endswith('-4bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,)}\n",
    "                model_name = model_name[:-len('-4bit')]\n",
    "            else:\n",
    "                kwargs = {}\n",
    "\n",
    "            model_id = f'mistralai/{model_name}'\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id, device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                device_map='auto',\n",
    "                max_memory={0: '80GIB'},\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        elif 'falcon' in model_name:\n",
    "            model_id = f'tiiuae/{model_name}'\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id, device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                load_in_8bit=True,)}\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                trust_remote_code=True,\n",
    "                device_map='auto',\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.stop_sequences = stop_sequences + [self.tokenizer.eos_token]\n",
    "        self.token_limit = 4096 if 'Llama-2' in model_name or \"Llama-3\" in model_name else 2048\n",
    "\n",
    "    def predict(self, input_data, temperature, return_full=False):\n",
    "\n",
    "        # Implement prediction.\n",
    "        inputs = self.tokenizer(input_data, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        if 'llama' in self.model_name.lower() or 'falcon' in self.model_name or 'mistral' in self.model_name.lower():\n",
    "            if 'token_type_ids' in inputs:  # Some HF models have changed.\n",
    "                del inputs['token_type_ids']\n",
    "            pad_token_id = self.tokenizer.eos_token_id\n",
    "        else:\n",
    "            pad_token_id = None\n",
    "\n",
    "        if self.stop_sequences is not None:\n",
    "            stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(\n",
    "                stops=self.stop_sequences,\n",
    "                initial_length=len(inputs['input_ids'][0]),\n",
    "                tokenizer=self.tokenizer)])\n",
    "        else:\n",
    "            stopping_criteria = None\n",
    "\n",
    "        logging.debug('temperature: %f', temperature)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                output_hidden_states=True,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "            )\n",
    "\n",
    "        if len(outputs.sequences[0]) > self.token_limit:\n",
    "            raise ValueError(\n",
    "                'Generation exceeding token limit %d > %d',\n",
    "                len(outputs.sequences[0]), self.token_limit)\n",
    "\n",
    "        full_answer = self.tokenizer.decode(\n",
    "            outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "        if return_full:\n",
    "            return full_answer\n",
    "\n",
    "        # For some models, we need to remove the input_data from the answer.\n",
    "        if full_answer.startswith(input_data):\n",
    "            input_data_offset = len(input_data)\n",
    "        else:\n",
    "            #raise ValueError('Have not tested this in a while.')\n",
    "            logging.error(f\"Full answer should start from input_data. Setting input_data offset to 0\")\n",
    "            logging.error(f\"Full answer is {full_answer}\")\n",
    "            logging.error(f\"Input data is {input_data}\")\n",
    "            input_data_offset = 0\n",
    "\n",
    "        # Remove input from answer.\n",
    "        answer = full_answer[input_data_offset:]\n",
    "\n",
    "        # Remove stop_words from answer.\n",
    "        stop_at = len(answer)\n",
    "        sliced_answer = answer\n",
    "        if self.stop_sequences is not None:\n",
    "            for stop in self.stop_sequences:\n",
    "                if answer.endswith(stop):\n",
    "                    stop_at = len(answer) - len(stop)\n",
    "                    sliced_answer = answer[:stop_at]\n",
    "                    break\n",
    "            if not all([stop not in sliced_answer for stop in self.stop_sequences]):\n",
    "                error_msg = 'Error: Stop words not removed successfully!'\n",
    "                error_msg += f'Answer: >{answer}< '\n",
    "                error_msg += f'Sliced Answer: >{sliced_answer}<'\n",
    "                if 'falcon' not in self.model_name.lower():\n",
    "                    raise ValueError(error_msg)\n",
    "                else:\n",
    "                    logging.error(error_msg)\n",
    "\n",
    "        # Remove whitespaces from answer (in particular from beginning.)\n",
    "        sliced_answer = sliced_answer.strip()\n",
    "\n",
    "        # Get the number of tokens until the stop word comes up.\n",
    "        # Note: Indexing with `stop_at` already excludes the stop_token.\n",
    "        # Note: It's important we do this with full answer, since there might be\n",
    "        # non-trivial interactions between the input_data and generated part\n",
    "        # in tokenization (particularly around whitespaces.)\n",
    "        token_stop_index = self.tokenizer(full_answer[:input_data_offset + stop_at], return_tensors=\"pt\")['input_ids'].shape[1]\n",
    "        n_input_token = len(inputs['input_ids'][0])\n",
    "        n_generated = token_stop_index - n_input_token\n",
    "\n",
    "        if n_generated == 0:\n",
    "            logging.warning('Only stop_words were generated. For likelihoods and embeddings, taking stop word instead.')\n",
    "            n_generated = 1\n",
    "\n",
    "        # Get the last hidden state (last layer) and the last token's embedding of the answer.\n",
    "        # Note: We do not want this to be the stop token.\n",
    "\n",
    "        # outputs.hidden_state is a tuple of len = n_generated_tokens.\n",
    "        # The first hidden state is for the input tokens and is of shape\n",
    "        #     (n_layers) x (batch_size, input_size, hidden_size).\n",
    "        # (Note this includes the first generated token!)\n",
    "        # The remaining hidden states are for the remaining generated tokens and is of shape\n",
    "        #    (n_layers) x (batch_size, 1, hidden_size).\n",
    "\n",
    "        # Note: The output embeddings have the shape (batch_size, generated_length, hidden_size).\n",
    "        # We do not get embeddings for input_data! We thus subtract the n_tokens_in_input from\n",
    "        # token_stop_index to arrive at the right output.\n",
    "\n",
    "        if 'decoder_hidden_states' in outputs.keys():\n",
    "            hidden = outputs.decoder_hidden_states\n",
    "        else:\n",
    "            hidden = outputs.hidden_states\n",
    "\n",
    "        if len(hidden) == 1:\n",
    "            logging.warning(\n",
    "                'Taking first and only generation for hidden! '\n",
    "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
    "                'last_token: %s, generation was: %s',\n",
    "                n_generated, n_input_token, token_stop_index,\n",
    "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
    "                full_answer,\n",
    "                )\n",
    "            last_input = hidden[0]\n",
    "        elif ((n_generated - 1) >= len(hidden)):\n",
    "            # If access idx is larger/equal.\n",
    "            logging.error(\n",
    "                'Taking last state because n_generated is too large'\n",
    "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
    "                'last_token: %s, generation was: %s, slice_answer: %s',\n",
    "                n_generated, n_input_token, token_stop_index,\n",
    "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
    "                full_answer, sliced_answer\n",
    "                )\n",
    "            last_input = hidden[-1]\n",
    "        else:\n",
    "            last_input = hidden[n_generated - 1]\n",
    "\n",
    "        # Then access last layer for input\n",
    "        last_layer = last_input[-1]\n",
    "        # Then access last token in input.\n",
    "        last_token_embedding = last_layer[:, -1, :].cpu()\n",
    "\n",
    "        # Get log_likelihoods.\n",
    "        # outputs.scores are the logits for the generated token.\n",
    "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
    "        # Each entry is shape (bs, vocabulary size).\n",
    "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
    "        transition_scores = self.model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
    "\n",
    "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
    "        if len(log_likelihoods) == 1:\n",
    "            logging.warning('Taking first and only generation for log likelihood!')\n",
    "            log_likelihoods = log_likelihoods\n",
    "        else:\n",
    "            log_likelihoods = log_likelihoods[:n_generated]\n",
    "\n",
    "        if len(log_likelihoods) == self.max_new_tokens:\n",
    "            logging.warning('Generation interrupted by max_token limit.')\n",
    "\n",
    "        if len(log_likelihoods) == 0:\n",
    "            raise ValueError\n",
    "\n",
    "        return sliced_answer, log_likelihoods, last_token_embedding\n",
    "\n",
    "    def get_p_true(self, input_data):\n",
    "        \"\"\"Get the probability of the model anwering A (True) for the given input.\"\"\"\n",
    "\n",
    "        input_data += ' A'\n",
    "        tokenized_prompt_true = self.tokenizer(input_data, return_tensors='pt').to('cuda')['input_ids']\n",
    "        # The computation of the negative log likelihoods follows:\n",
    "        # https://huggingface.co/docs/transformers/perplexity.\n",
    "\n",
    "        target_ids_true = tokenized_prompt_true.clone()\n",
    "        # Set all target_ids except the last one to -100.\n",
    "        target_ids_true[0, :-1] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output_true = self.model(tokenized_prompt_true, labels=target_ids_true)\n",
    "\n",
    "        loss_true = model_output_true.loss\n",
    "\n",
    "        return -loss_true.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4442d94",
   "metadata": {},
   "source": [
    "# suite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ad6753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIEF_PROMPTS = {\n",
    "    'default': \"Answer the following question as briefly as possible.\\n\",\n",
    "    'chat': 'Answer the following question in a single brief but complete sentence.\\n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c3bff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prompt = get_make_prompt()\n",
    "BRIEF = BRIEF_PROMPTS['default']\n",
    "prompt = construct_fewshot_prompt_from_indices(\n",
    "train_dataset, prompt_indices, BRIEF, False, make_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96642110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model_name, model_max_new_tokens):\n",
    "    if 'llama' in model_name.lower() or 'falcon' in model_name or 'mistral' in model_name.lower():\n",
    "        model = HuggingfaceModel(\n",
    "            model_name, stop_sequences='default',\n",
    "            max_new_tokens=model_max_new_tokens)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model_name `{model_name}`.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9f517e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteEntryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-69624c99-36de1b2e4800834f41c4aea8;f5bc6183-9196-41b3-b5ab-2a85bb839999)\n\nEntry Not Found for url: https://huggingface.co/api/models/tiiuae/falcon-7b/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:657\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '404 Not Found' for url 'https://huggingface.co/api/models/tiiuae/falcon-7b/tree/main/additional_chat_templates?recursive=false&expand=false'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRemoteEntryNotFoundError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mfalcon-7b\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m model_max_new_tokens = \u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43minit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_max_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36minit_model\u001b[39m\u001b[34m(model_name, model_max_new_tokens)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit_model\u001b[39m(model_name, model_max_new_tokens):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mllama\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name.lower() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mfalcon\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmistral\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name.lower():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         model = \u001b[43mHuggingfaceModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_max_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnknown model_name `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mHuggingfaceModel.__init__\u001b[39m\u001b[34m(self, model_name, stop_sequences, max_new_tokens)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mfalcon\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[32m    175\u001b[39m     model_id = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtiiuae/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     kwargs = {\u001b[33m'\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m'\u001b[39m: BitsAndBytesConfig(\n\u001b[32m    181\u001b[39m         load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m,)}\n\u001b[32m    183\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m    184\u001b[39m         model_id,\n\u001b[32m    185\u001b[39m         trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    186\u001b[39m         device_map=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    187\u001b[39m         **kwargs,\n\u001b[32m    188\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1156\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1153\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1154\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1155\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m   1159\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2039\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2035\u001b[39m             vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m   2036\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2037\u001b[39m             )\n\u001b[32m   2038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2039\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2045\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2046\u001b[39m         template = template.removesuffix(\u001b[33m\"\u001b[39m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2047\u001b[39m         vocab_files[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jinja\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/transformers/utils/hub.py:169\u001b[39m, in \u001b[36mlist_repo_templates\u001b[39m\u001b[34m(repo_id, local_files_only, revision, cache_dir, token)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveprefix\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_repo_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_TEMPLATE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/huggingface_hub/hf_api.py:3297\u001b[39m, in \u001b[36mlist_repo_tree\u001b[39m\u001b[34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[39m\n\u001b[32m   3266\u001b[39m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[32m   3267\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_repo_commits\u001b[39m(\n\u001b[32m   3268\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3274\u001b[39m     formatted: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3275\u001b[39m ) -> List[GitCommitInfo]:\n\u001b[32m   3276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3277\u001b[39m \u001b[33;03m    Get the list of commits of a given revision for a repo on the Hub.\u001b[39;00m\n\u001b[32m   3278\u001b[39m \n\u001b[32m   3279\u001b[39m \u001b[33;03m    Commits are sorted by date (last commit first).\u001b[39;00m\n\u001b[32m   3280\u001b[39m \n\u001b[32m   3281\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   3282\u001b[39m \u001b[33;03m        repo_id (`str`):\u001b[39;00m\n\u001b[32m   3283\u001b[39m \u001b[33;03m            A namespace (user or an organization) and a repo name separated by a `/`.\u001b[39;00m\n\u001b[32m   3284\u001b[39m \u001b[33;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[32m   3285\u001b[39m \u001b[33;03m            Set to `\"dataset\"` or `\"space\"` if listing commits from a dataset or a Space, `None` or `\"model\"` if\u001b[39;00m\n\u001b[32m   3286\u001b[39m \u001b[33;03m            listing from a model. Default is `None`.\u001b[39;00m\n\u001b[32m   3287\u001b[39m \u001b[33;03m        token (`bool` or `str`, *optional*):\u001b[39;00m\n\u001b[32m   3288\u001b[39m \u001b[33;03m            A valid user access token (string). Defaults to the locally saved\u001b[39;00m\n\u001b[32m   3289\u001b[39m \u001b[33;03m            token, which is the recommended method for authentication (see\u001b[39;00m\n\u001b[32m   3290\u001b[39m \u001b[33;03m            https://huggingface.co/docs/huggingface_hub/quick-start#authentication).\u001b[39;00m\n\u001b[32m   3291\u001b[39m \u001b[33;03m            To disable authentication, pass `False`.\u001b[39;00m\n\u001b[32m   3292\u001b[39m \u001b[33;03m        revision (`str`, *optional*):\u001b[39;00m\n\u001b[32m   3293\u001b[39m \u001b[33;03m            The git revision to commit from. Defaults to the head of the `\"main\"` branch.\u001b[39;00m\n\u001b[32m   3294\u001b[39m \u001b[33;03m        formatted (`bool`):\u001b[39;00m\n\u001b[32m   3295\u001b[39m \u001b[33;03m            Whether to return the HTML-formatted title and description of the commits. Defaults to False.\u001b[39;00m\n\u001b[32m   3296\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m3297\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m   3298\u001b[39m \u001b[33;03m    ```py\u001b[39;00m\n\u001b[32m   3299\u001b[39m \u001b[33;03m    >>> from huggingface_hub import HfApi\u001b[39;00m\n\u001b[32m   3300\u001b[39m \u001b[33;03m    >>> api = HfApi()\u001b[39;00m\n\u001b[32m   3301\u001b[39m \n\u001b[32m   3302\u001b[39m \u001b[33;03m    # Commits are sorted by date (last commit first)\u001b[39;00m\n\u001b[32m   3303\u001b[39m \u001b[33;03m    >>> initial_commit = api.list_repo_commits(\"gpt2\")[-1]\u001b[39;00m\n\u001b[32m   3304\u001b[39m \n\u001b[32m   3305\u001b[39m \u001b[33;03m    # Initial commit is always a system commit containing the `.gitattributes` file.\u001b[39;00m\n\u001b[32m   3306\u001b[39m \u001b[33;03m    >>> initial_commit\u001b[39;00m\n\u001b[32m   3307\u001b[39m \u001b[33;03m    GitCommitInfo(\u001b[39;00m\n\u001b[32m   3308\u001b[39m \u001b[33;03m        commit_id='9b865efde13a30c13e0a33e536cf3e4a5a9d71d8',\u001b[39;00m\n\u001b[32m   3309\u001b[39m \u001b[33;03m        authors=['system'],\u001b[39;00m\n\u001b[32m   3310\u001b[39m \u001b[33;03m        created_at=datetime.datetime(2019, 2, 18, 10, 36, 15, tzinfo=datetime.timezone.utc),\u001b[39;00m\n\u001b[32m   3311\u001b[39m \u001b[33;03m        title='initial commit',\u001b[39;00m\n\u001b[32m   3312\u001b[39m \u001b[33;03m        message='',\u001b[39;00m\n\u001b[32m   3313\u001b[39m \u001b[33;03m        formatted_title=None,\u001b[39;00m\n\u001b[32m   3314\u001b[39m \u001b[33;03m        formatted_message=None\u001b[39;00m\n\u001b[32m   3315\u001b[39m \u001b[33;03m    )\u001b[39;00m\n\u001b[32m   3316\u001b[39m \n\u001b[32m   3317\u001b[39m \u001b[33;03m    # Create an empty branch by deriving from initial commit\u001b[39;00m\n\u001b[32m   3318\u001b[39m \u001b[33;03m    >>> api.create_branch(\"gpt2\", \"new_empty_branch\", revision=initial_commit.commit_id)\u001b[39;00m\n\u001b[32m   3319\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m   3320\u001b[39m \n\u001b[32m   3321\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   3322\u001b[39m \u001b[33;03m        List[[`GitCommitInfo`]]: list of objects containing information about the commits for a repo on the Hub.\u001b[39;00m\n\u001b[32m   3323\u001b[39m \n\u001b[32m   3324\u001b[39m \u001b[33;03m    Raises:\u001b[39;00m\n\u001b[32m   3325\u001b[39m \u001b[33;03m        [`~utils.RepositoryNotFoundError`]:\u001b[39;00m\n\u001b[32m   3326\u001b[39m \u001b[33;03m            If repository is not found (error 404): wrong repo_id/repo_type, private but not authenticated or repo\u001b[39;00m\n\u001b[32m   3327\u001b[39m \u001b[33;03m            does not exist.\u001b[39;00m\n\u001b[32m   3328\u001b[39m \u001b[33;03m        [`~utils.RevisionNotFoundError`]:\u001b[39;00m\n\u001b[32m   3329\u001b[39m \u001b[33;03m            If revision is not found (error 404) on the repo.\u001b[39;00m\n\u001b[32m   3330\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3331\u001b[39m     repo_type = repo_type \u001b[38;5;129;01mor\u001b[39;00m constants.REPO_TYPE_MODEL\n\u001b[32m   3332\u001b[39m     revision = quote(revision, safe=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m constants.DEFAULT_REVISION\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/huggingface_hub/utils/_pagination.py:37\u001b[39m, in \u001b[36mpaginate\u001b[39m\u001b[34m(path, params, headers)\u001b[39m\n\u001b[32m     35\u001b[39m session = get_session()\n\u001b[32m     36\u001b[39m r = session.get(path, params=params, headers=headers)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m r.json()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Follow pages\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Next link already contains query params\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:671\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mRemoteEntryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-69624c99-36de1b2e4800834f41c4aea8;f5bc6183-9196-41b3-b5ab-2a85bb839999)\n\nEntry Not Found for url: https://huggingface.co/api/models/tiiuae/falcon-7b/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\""
     ]
    }
   ],
   "source": [
    "model_name = \"falcon-7b\"\n",
    "model_max_new_tokens = 100\n",
    "model = init_model(model_name, model_max_new_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
