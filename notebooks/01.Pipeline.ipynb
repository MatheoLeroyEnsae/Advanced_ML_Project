{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585ac353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from datasets) (2.4.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/python/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/python/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/python/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/python/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.3.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/python/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/python/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/python/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.13/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/python/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-1.3.1-py3-none-any.whl (533 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.4/533.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, typer-slim, shellingham, httpcore, hf-xet, fsspec, filelock, dill, multiprocess, httpx, huggingface-hub, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\n",
      "\u001b[2K    Found existing installation: fsspec 2025.12.0\n",
      "\u001b[2K    Uninstalling fsspec-2025.12.0:\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.12.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [datasets]/12\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2025.12.0 requires fsspec==2025.12.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.4.2 dill-0.4.0 filelock-3.20.3 fsspec-2025.10.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.3.1 multiprocess-0.70.18 shellingham-1.5.4 typer-slim-0.21.1 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/python/lib/python3.13/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: pyyaml in /opt/python/lib/python3.13/site-packages (from accelerate) (6.0.3)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Downloading torch-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/python/lib/python3.13/site-packages (from accelerate) (1.3.1)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/python/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: anyio in /opt/python/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/python/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/python/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/python/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/python/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading triton-3.5.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->accelerate)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/python/lib/python3.13/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.3.1)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading torch-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, safetensors, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, accelerate\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/22\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.28.9━━━━━━\u001b[0m \u001b[32m 7/22\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.28.9:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/22\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.28.9━━━━━━━━━━━━\u001b[0m \u001b[32m 8/22\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [accelerate]2\u001b[0m [accelerate]lver-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 safetensors-0.7.0 sympy-1.14.0 torch-2.9.1 triton-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers) (3.20.3)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/python/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (2026.1.4)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.3.1\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.3.1:\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.3.1\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.22.2 transformers-4.57.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/python/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/python/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/python/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/python/lib/python3.13/site-packages (from evaluate) (4.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from evaluate) (2.4.0)\n",
      "Requirement already satisfied: dill in /opt/python/lib/python3.13/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/python/lib/python3.13/site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/python/lib/python3.13/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/python/lib/python3.13/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/python/lib/python3.13/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /opt/python/lib/python3.13/site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/python/lib/python3.13/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/python/lib/python3.13/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/python/lib/python3.13/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/python/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/python/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/python/lib/python3.13/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/python/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/python/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests>=2.19.0->evaluate) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/lib/python3.13/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/lib/python3.13/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.13/site-packages (from pandas->evaluate) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install accelerate\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db8de2",
   "metadata": {},
   "source": [
    "# Funcction s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fe912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import datasets\n",
    "import random \n",
    "from evaluate import load\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ace869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(dataset_name, seed, add_options=None):\n",
    "    \"\"\"Load dataset.\"\"\"\n",
    "\n",
    "    train_dataset, validation_dataset = None, None\n",
    "\n",
    "    if dataset_name == \"trivia_qa\":\n",
    "            dataset = datasets.load_dataset('TimoImhof/TriviaQA-in-SQuAD-format')['unmodified']\n",
    "            dataset = dataset.train_test_split(test_size=0.2, seed=seed)\n",
    "            train_dataset = dataset['train']\n",
    "            validation_dataset = dataset['test']\n",
    "    return train_dataset, validation_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36063cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modified_100_percent split: 100%|██████████| 15368/15368 [00:00<00:00, 107585.21 examples/s]\n",
      "Generating modified_30_percent split: 100%|██████████| 15368/15368 [00:00<00:00, 115117.66 examples/s]\n",
      "Generating unmodified split: 100%|██████████| 15368/15368 [00:00<00:00, 130681.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset = load_ds( \"trivia_qa\", 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a407c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    \"\"\"Get indices of answerable and unanswerable questions.\"\"\"\n",
    "\n",
    "    def clen(ex):\n",
    "        return len(ex[\"answers\"][\"text\"])\n",
    "\n",
    "    answerable_indices = [i for i, ex in enumerate(dataset) if clen(ex) > 0]\n",
    "    unanswerable_indices = [i for i, ex in enumerate(dataset) if clen(ex) == 0]\n",
    "\n",
    "    # union == full dataset\n",
    "    assert set(answerable_indices) | set(\n",
    "        unanswerable_indices) == set(range(len(dataset)))\n",
    "    # no overlap\n",
    "    assert set(answerable_indices) - \\\n",
    "        set(unanswerable_indices) == set(answerable_indices)\n",
    "\n",
    "    return answerable_indices, unanswerable_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930c9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_indices, unanswerable_indices = split_dataset(train_dataset)\n",
    "\n",
    "unanswerable_indices = []\n",
    "val_answerable, val_unanswerable = split_dataset(validation_dataset)\n",
    "del val_unanswerable\n",
    "validation_dataset = [validation_dataset[i] for i in val_answerable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ee695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_few_shot = 3\n",
    "prompt_indices = random.sample(answerable_indices, num_few_shot)\n",
    "remaining_answerable = list(set(answerable_indices) - set(prompt_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f2b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_make_prompt():\n",
    "  def make_prompt(context, question, answer, brief, brief_always):\n",
    "            prompt = ''\n",
    "            if brief_always:\n",
    "                prompt += brief\n",
    "            if (context is not None):\n",
    "                prompt += f\"Context: {context}\\n\"\n",
    "            prompt += f\"Question: {question}\\n\"\n",
    "            if answer:\n",
    "                prompt += f\"Answer: {answer}\\n\\n\"\n",
    "            else:\n",
    "                prompt += 'Answer:'\n",
    "            return prompt\n",
    "        \n",
    "  return make_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2bbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_fewshot_prompt_from_indices(dataset, example_indices, brief, brief_always, make_prompt):\n",
    "    \"\"\"Given a dataset and indices, construct a fewshot prompt.\"\"\"\n",
    "    if not brief_always:\n",
    "        prompt = brief\n",
    "    else:\n",
    "        prompt = ''\n",
    "\n",
    "    for example_index in example_indices:\n",
    "\n",
    "        example = dataset[example_index]\n",
    "        context = example[\"context\"]\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "        prompt = prompt + make_prompt(context, question, answer, brief, brief_always)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bbe0d",
   "metadata": {},
   "source": [
    "# HugginFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fcd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Text\n",
    "\n",
    "\n",
    "STOP_SEQUENCES = ['\\n\\n\\n\\n', '\\n\\n\\n', '\\n\\n', '\\n', 'Question:', 'Context:']\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "\n",
    "    stop_sequences: List[Text]\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, input_data, temperature):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_p_true(self, input_data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7817ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement HuggingfaceModel models.\"\"\"\n",
    "import copy\n",
    "import logging\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import StoppingCriteria\n",
    "from transformers import StoppingCriteriaList\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    \"\"\"Stop generations when they match a particular text or token.\"\"\"\n",
    "    def __init__(self, stops, tokenizer, match_on='text', initial_length=None):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.initial_length = initial_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.match_on = match_on\n",
    "        if self.match_on == 'tokens':\n",
    "            self.stops = [torch.tensor(self.tokenizer.encode(i)).to('cuda') for i in self.stops]\n",
    "            print(self.stops)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        del scores  # `scores` arg is required by StoppingCriteria but unused by us.\n",
    "        for stop in self.stops:\n",
    "            if self.match_on == 'text':\n",
    "                generation = self.tokenizer.decode(input_ids[0][self.initial_length:], skip_special_tokens=False)\n",
    "                match = stop in generation\n",
    "            elif self.match_on == 'tokens':\n",
    "                # Can be dangerous due to tokenizer ambiguities.\n",
    "                match = stop in input_ids[0][-len(stop):]\n",
    "            else:\n",
    "                raise\n",
    "            if match:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def remove_split_layer(device_map_in):\n",
    "    \"\"\"Modify device maps s.t. individual layers are not spread across devices.\"\"\"\n",
    "\n",
    "    device_map = copy.deepcopy(device_map_in)\n",
    "    destinations = list(device_map.keys())\n",
    "\n",
    "    counts = Counter(['.'.join(i.split('.')[:2]) for i in destinations])\n",
    "\n",
    "    found_split = False\n",
    "    for layer, count in counts.items():\n",
    "        if count == 1:\n",
    "            continue\n",
    "\n",
    "        if found_split:\n",
    "            # Only triggers if we find more than one split layer.\n",
    "            raise ValueError(\n",
    "                'More than one split layer.\\n'\n",
    "                f'Currently at layer {layer}.\\n'\n",
    "                f'In map: {device_map_in}\\n'\n",
    "                f'Out map: {device_map}\\n')\n",
    "\n",
    "        logging.info(f'Split layer is {layer}.')\n",
    "\n",
    "        # Remove split for that layer.\n",
    "        for name in list(device_map.keys()):\n",
    "            if name.startswith(layer):\n",
    "                print(f'pop {name}')\n",
    "                device = device_map.pop(name)\n",
    "\n",
    "        device_map[layer] = device\n",
    "        found_split = True\n",
    "\n",
    "    return device_map\n",
    "\n",
    "\n",
    "class HuggingfaceModel(BaseModel):\n",
    "    \"\"\"Hugging Face Model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name, stop_sequences=None, max_new_tokens=None):\n",
    "        if max_new_tokens is None:\n",
    "            raise\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        if stop_sequences == 'default':\n",
    "            stop_sequences = STOP_SEQUENCES\n",
    "\n",
    "        if 'llama' in model_name.lower():\n",
    "            if model_name.endswith('-8bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,)}\n",
    "                model_name = model_name[:-len('-8bit')]\n",
    "                eightbit = True\n",
    "            else:\n",
    "                kwargs = {}\n",
    "                eightbit = False\n",
    "\n",
    "            if 'Llama-2' in model_name or \"Llama-3\" in model_name:\n",
    "                base = 'meta-llama'\n",
    "                if 'Llama-2' in model_name:\n",
    "                    model_name = model_name + '-hf'\n",
    "            else:\n",
    "                base = 'huggyllama'\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                f\"{base}/{model_name}\", device_map=\"auto\",\n",
    "                token_type_ids=None)\n",
    "\n",
    "            llama65b = '65b' in model_name and base == 'huggyllama'\n",
    "            llama2_70b = '70b' in model_name and base == 'meta-llama'\n",
    "\n",
    "            if ('7b' in model_name or \"8B\" in model_name or '13b' in model_name) or eightbit:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    f\"{base}/{model_name}\", device_map=\"auto\",\n",
    "                    max_memory={0: '80GIB'}, **kwargs,)\n",
    "\n",
    "            elif llama2_70b or llama65b:\n",
    "                path = snapshot_download(\n",
    "                    repo_id=f'{base}/{model_name}',\n",
    "                    allow_patterns=['*.json', '*.model', '*.safetensors'],\n",
    "                    ignore_patterns=['pytorch_model.bin.index.json']\n",
    "                )\n",
    "\n",
    "                config = AutoConfig.from_pretrained(f\"{base}/{model_name}\")\n",
    "                with accelerate.init_empty_weights():\n",
    "                    self.model = AutoModelForCausalLM.from_config(config)\n",
    "                self.model.tie_weights()\n",
    "                max_mem = 15 * 4686198491\n",
    "\n",
    "                device_map = accelerate.infer_auto_device_map(\n",
    "                    self.model.model,\n",
    "                    max_memory={0: max_mem, 1: max_mem},\n",
    "                    dtype='float16'\n",
    "                )\n",
    "                device_map = remove_split_layer(device_map)\n",
    "                full_model_device_map = {f\"model.{k}\": v for k, v in device_map.items()}\n",
    "                full_model_device_map[\"lm_head\"] = 0\n",
    "\n",
    "                self.model = accelerate.load_checkpoint_and_dispatch(\n",
    "                    self.model, path, device_map=full_model_device_map,\n",
    "                    dtype='float16', skip_keys='past_key_values')\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "        elif 'mistral' in model_name.lower():\n",
    "\n",
    "            if model_name.endswith('-8bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,)}\n",
    "                model_name = model_name[:-len('-8bit')]\n",
    "            if model_name.endswith('-4bit'):\n",
    "                kwargs = {'quantization_config': BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,)}\n",
    "                model_name = model_name[:-len('-4bit')]\n",
    "            else:\n",
    "                kwargs = {}\n",
    "\n",
    "            model_id = f'mistralai/{model_name}'\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id, device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                device_map='auto',\n",
    "                max_memory={0: '80GIB'},\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        elif 'falcon' in model_name:\n",
    "            \n",
    "\n",
    "            model_id = f'tiiuae/{model_name}'\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id, device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "            )\n",
    "            \n",
    "        elif 'Falcon' in model_name:\n",
    "            model_id = f'tiiuae/{model_name}'\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id, device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "            )\n",
    "        \n",
    "        elif \"Qwen\" in model_name:\n",
    "\n",
    "            model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_id,device_map='auto', token_type_ids=None,\n",
    "                clean_up_tokenization_spaces=False)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.stop_sequences = stop_sequences + [self.tokenizer.eos_token]\n",
    "        self.token_limit = 4096 if 'Llama-2' in model_name or \"Llama-3\" in model_name else 2048\n",
    "\n",
    "    def predict(self, input_data, temperature, return_full=False):\n",
    "\n",
    "        # Implement prediction.\n",
    "        inputs = self.tokenizer(input_data, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        if 'llama' in self.model_name.lower() or 'falcon' in self.model_name or 'mistral' in self.model_name.lower():\n",
    "            if 'token_type_ids' in inputs:  # Some HF models have changed.\n",
    "                del inputs['token_type_ids']\n",
    "            pad_token_id = self.tokenizer.eos_token_id\n",
    "        else:\n",
    "            pad_token_id = None\n",
    "\n",
    "        if self.stop_sequences is not None:\n",
    "            stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(\n",
    "                stops=self.stop_sequences,\n",
    "                initial_length=len(inputs['input_ids'][0]),\n",
    "                tokenizer=self.tokenizer)])\n",
    "        else:\n",
    "            stopping_criteria = None\n",
    "\n",
    "        logging.debug('temperature: %f', temperature)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                output_hidden_states=True,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                pad_token_id=pad_token_id,\n",
    "            )\n",
    "\n",
    "        if len(outputs.sequences[0]) > self.token_limit:\n",
    "            raise ValueError(\n",
    "                'Generation exceeding token limit %d > %d',\n",
    "                len(outputs.sequences[0]), self.token_limit)\n",
    "\n",
    "        full_answer = self.tokenizer.decode(\n",
    "            outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "        if return_full:\n",
    "            return full_answer\n",
    "\n",
    "        # For some models, we need to remove the input_data from the answer.\n",
    "        if full_answer.startswith(input_data):\n",
    "            input_data_offset = len(input_data)\n",
    "        else:\n",
    "            #raise ValueError('Have not tested this in a while.')\n",
    "            logging.error(f\"Full answer should start from input_data. Setting input_data offset to 0\")\n",
    "            logging.error(f\"Full answer is {full_answer}\")\n",
    "            logging.error(f\"Input data is {input_data}\")\n",
    "            input_data_offset = 0\n",
    "\n",
    "        # Remove input from answer.\n",
    "        answer = full_answer[input_data_offset:]\n",
    "\n",
    "        # Remove stop_words from answer.\n",
    "        stop_at = len(answer)\n",
    "        sliced_answer = answer\n",
    "        if self.stop_sequences is not None:\n",
    "            for stop in self.stop_sequences:\n",
    "                if answer.endswith(stop):\n",
    "                    stop_at = len(answer) - len(stop)\n",
    "                    sliced_answer = answer[:stop_at]\n",
    "                    break\n",
    "            if not all([stop not in sliced_answer for stop in self.stop_sequences]):\n",
    "                error_msg = 'Error: Stop words not removed successfully!'\n",
    "                error_msg += f'Answer: >{answer}< '\n",
    "                error_msg += f'Sliced Answer: >{sliced_answer}<'\n",
    "                if 'falcon' not in self.model_name.lower():\n",
    "                    raise ValueError(error_msg)\n",
    "                else:\n",
    "                    logging.error(error_msg)\n",
    "\n",
    "        # Remove whitespaces from answer (in particular from beginning.)\n",
    "        sliced_answer = sliced_answer.strip()\n",
    "\n",
    "        # Get the number of tokens until the stop word comes up.\n",
    "        # Note: Indexing with `stop_at` already excludes the stop_token.\n",
    "        # Note: It's important we do this with full answer, since there might be\n",
    "        # non-trivial interactions between the input_data and generated part\n",
    "        # in tokenization (particularly around whitespaces.)\n",
    "        token_stop_index = self.tokenizer(full_answer[:input_data_offset + stop_at], return_tensors=\"pt\")['input_ids'].shape[1]\n",
    "        n_input_token = len(inputs['input_ids'][0])\n",
    "        n_generated = token_stop_index - n_input_token\n",
    "\n",
    "        if n_generated == 0:\n",
    "            logging.warning('Only stop_words were generated. For likelihoods and embeddings, taking stop word instead.')\n",
    "            n_generated = 1\n",
    "\n",
    "        # Get the last hidden state (last layer) and the last token's embedding of the answer.\n",
    "        # Note: We do not want this to be the stop token.\n",
    "\n",
    "        # outputs.hidden_state is a tuple of len = n_generated_tokens.\n",
    "        # The first hidden state is for the input tokens and is of shape\n",
    "        #     (n_layers) x (batch_size, input_size, hidden_size).\n",
    "        # (Note this includes the first generated token!)\n",
    "        # The remaining hidden states are for the remaining generated tokens and is of shape\n",
    "        #    (n_layers) x (batch_size, 1, hidden_size).\n",
    "\n",
    "        # Note: The output embeddings have the shape (batch_size, generated_length, hidden_size).\n",
    "        # We do not get embeddings for input_data! We thus subtract the n_tokens_in_input from\n",
    "        # token_stop_index to arrive at the right output.\n",
    "\n",
    "        if 'decoder_hidden_states' in outputs.keys():\n",
    "            hidden = outputs.decoder_hidden_states\n",
    "        else:\n",
    "            hidden = outputs.hidden_states\n",
    "\n",
    "        if len(hidden) == 1:\n",
    "            logging.warning(\n",
    "                'Taking first and only generation for hidden! '\n",
    "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
    "                'last_token: %s, generation was: %s',\n",
    "                n_generated, n_input_token, token_stop_index,\n",
    "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
    "                full_answer,\n",
    "                )\n",
    "            last_input = hidden[0]\n",
    "        elif ((n_generated - 1) >= len(hidden)):\n",
    "            # If access idx is larger/equal.\n",
    "            logging.error(\n",
    "                'Taking last state because n_generated is too large'\n",
    "                'n_generated: %d, n_input_token: %d, token_stop_index %d, '\n",
    "                'last_token: %s, generation was: %s, slice_answer: %s',\n",
    "                n_generated, n_input_token, token_stop_index,\n",
    "                self.tokenizer.decode(outputs['sequences'][0][-1]),\n",
    "                full_answer, sliced_answer\n",
    "                )\n",
    "            last_input = hidden[-1]\n",
    "        else:\n",
    "            last_input = hidden[n_generated - 1]\n",
    "\n",
    "        # Then access last layer for input\n",
    "        last_layer = last_input[-1]\n",
    "        # Then access last token in input.\n",
    "        last_token_embedding = last_layer[:, -1, :].cpu()\n",
    "\n",
    "        # Get log_likelihoods.\n",
    "        # outputs.scores are the logits for the generated token.\n",
    "        # outputs.scores is a tuple of len = n_generated_tokens.\n",
    "        # Each entry is shape (bs, vocabulary size).\n",
    "        # outputs.sequences is the sequence of all tokens: input and generated.\n",
    "        transition_scores = self.model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "        # Transition_scores[0] only contains the scores for the first generated tokens.\n",
    "\n",
    "        log_likelihoods = [score.item() for score in transition_scores[0]]\n",
    "        if len(log_likelihoods) == 1:\n",
    "            logging.warning('Taking first and only generation for log likelihood!')\n",
    "            log_likelihoods = log_likelihoods\n",
    "        else:\n",
    "            log_likelihoods = log_likelihoods[:n_generated]\n",
    "\n",
    "        if len(log_likelihoods) == self.max_new_tokens:\n",
    "            logging.warning('Generation interrupted by max_token limit.')\n",
    "\n",
    "        if len(log_likelihoods) == 0:\n",
    "            raise ValueError\n",
    "\n",
    "        return sliced_answer, log_likelihoods, last_token_embedding\n",
    "\n",
    "    def get_p_true(self, input_data):\n",
    "        \"\"\"Get the probability of the model anwering A (True) for the given input.\"\"\"\n",
    "\n",
    "        input_data += ' A'\n",
    "        tokenized_prompt_true = self.tokenizer(input_data, return_tensors='pt').to('cuda')['input_ids']\n",
    "        # The computation of the negative log likelihoods follows:\n",
    "        # https://huggingface.co/docs/transformers/perplexity.\n",
    "\n",
    "        target_ids_true = tokenized_prompt_true.clone()\n",
    "        # Set all target_ids except the last one to -100.\n",
    "        target_ids_true[0, :-1] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output_true = self.model(tokenized_prompt_true, labels=target_ids_true)\n",
    "\n",
    "        loss_true = model_output_true.loss\n",
    "\n",
    "        return -loss_true.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4442d94",
   "metadata": {},
   "source": [
    "# suite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad6753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIEF_PROMPTS = {\n",
    "    'default': \"Answer the following question as briefly as possible.\\n\",\n",
    "    'chat': 'Answer the following question in a single brief but complete sentence.\\n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c3bff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prompt = get_make_prompt()\n",
    "BRIEF = BRIEF_PROMPTS['default']\n",
    "prompt = construct_fewshot_prompt_from_indices(\n",
    "train_dataset, prompt_indices, BRIEF, False, make_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96642110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model_name, model_max_new_tokens):\n",
    "    if 'Qwen' in model_name or 'falcon' in model_name or 'mistral' in model_name.lower():\n",
    "        model = HuggingfaceModel(\n",
    "            model_name, stop_sequences='default',\n",
    "            max_new_tokens=model_max_new_tokens)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model_name `{model_name}`.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f517e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"falcon-7b-instruct\"\n",
    "model_name = \"Qwen2.5-1.5B-Instruct\"\n",
    "model_max_new_tokens = 100\n",
    "model = init_model(model_name, model_max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf0bb744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU libre  (GB): 11.50390625\n",
      "GPU total  (GB): 14.52947998046875\n"
     ]
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "print(\"GPU libre  (GB):\", free / 1024**3)\n",
    "print(\"GPU total  (GB):\", total / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca893ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true_num_fewshot = 4 \n",
    "num_generations = 5\n",
    "metric = 'squad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c3a8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(metric):\n",
    "    if metric == 'squad':\n",
    "\n",
    "        squad_metric = load(\"squad_v2\")\n",
    "\n",
    "        def metric_fct(response, example, *args, **kwargs):\n",
    "            # Compatibility with recomputation.\n",
    "            if 'id' in example:\n",
    "                exid = example['id']\n",
    "            elif 'id' in example['reference']:\n",
    "                exid = example['reference']['id']\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            prediction = {'prediction_text': response, 'no_answer_probability': 0.0, 'id': exid}\n",
    "            results = squad_metric.compute(\n",
    "                predictions=[prediction],\n",
    "                references=[get_reference(example)])\n",
    "            return 1.0 if (results['f1'] >= 50.0) else 0.0\n",
    "    return metric_fct\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "686b5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.47kB [00:00, 3.72MB/s]\n",
      "Downloading extra modules: 11.3kB [00:00, 7.55MB/s]\n"
     ]
    }
   ],
   "source": [
    "metric = get_metric(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1ab22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute p_true uncertainty metric.\"\"\"\n",
    "import logging\n",
    "\n",
    "\n",
    "def construct_few_shot_prompt(\n",
    "        *, model, dataset, indices, prompt, brief, brief_always, make_prompt,\n",
    "        num_generations, metric):\n",
    "    \"\"\"Construct few shot prompt for p_true uncertainty metric.\"\"\"\n",
    "\n",
    "    # Call model n_shots many times.\n",
    "    few_shot_prompt = []\n",
    "    all_responses = dict()\n",
    "    for it, i in enumerate(indices):\n",
    "        prompt_candidate = []\n",
    "        example = dataset[i]\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"context\"]\n",
    "        if it != 0:\n",
    "            prompt_candidate += ['\\n']\n",
    "        prompt_candidate += ['Question: ' + question]\n",
    "        prompt_candidate += ['\\nBrainstormed Answers: ']\n",
    "        current_question = make_prompt(context, question, None, brief, brief_always)\n",
    "        local_prompt = prompt + current_question\n",
    "        logging.info('P_TRUE >> Current Question: '.ljust(25) + current_question)\n",
    "\n",
    "        responses = []\n",
    "        for j in range(num_generations + 1):\n",
    "\n",
    "            if j == 0:\n",
    "                temperature = 0.1\n",
    "            else:\n",
    "                temperature = 1.0\n",
    "\n",
    "            response, _, _ = model.predict(local_prompt, temperature)\n",
    "            logging.info('P_TRUE >> Current Response: '.ljust(25) + response)\n",
    "\n",
    "            responses.append(response)\n",
    "            prompt_candidate += [f'{response.strip()} \\n']\n",
    "            if j == 0:\n",
    "                # Save most likely response and compute correctness metric for it.\n",
    "                most_likely_response = response\n",
    "                is_correct = metric(response, example, model)\n",
    "                answers = [answer for answer in example['answers']['text']]\n",
    "                logging.info('P_TRUE >> LOW-T >> true answer: '.ljust(35) + str(answers))\n",
    "                logging.info('P_TRUE >> LOW-T >> acc: '.ljust(35) + str(is_correct))\n",
    "\n",
    "        all_responses[i] = dict(\n",
    "            responses=responses, most_likely_response=most_likely_response,\n",
    "            is_correct=is_correct)\n",
    "\n",
    "        prompt_candidate += ['Possible answer: ' + most_likely_response + '\\n']\n",
    "        prompt_candidate += ['Is the possible answer:\\n']\n",
    "        prompt_candidate += ['A) True\\n']\n",
    "        prompt_candidate += ['B) False\\n']\n",
    "        prompt_candidate += ['The possible answer is:']\n",
    "        prompt_candidate += [' A' if is_correct else ' B']\n",
    "\n",
    "        prompt_len = len(model.tokenizer.encode(''.join(few_shot_prompt + prompt_candidate)))\n",
    "        # At test time, get a maximum of `num_generations * model.token_limit` extra tokens\n",
    "        # 200 buffer for question and 'Possible Answer'.\n",
    "        max_input_len = prompt_len + num_generations * model.max_new_tokens + 200\n",
    "\n",
    "        if max_input_len < model.token_limit:\n",
    "            few_shot_prompt.extend(prompt_candidate)\n",
    "        else:\n",
    "            logging.warning('Cutting of p_true prompt at length %d.', it)\n",
    "            break\n",
    "\n",
    "    return ''.join(few_shot_prompt), all_responses, it\n",
    "\n",
    "\n",
    "def calculate_p_true(\n",
    "        model, question, most_probable_answer, brainstormed_answers,\n",
    "        few_shot_prompt, hint=False):\n",
    "    \"\"\"Calculate p_true uncertainty metric.\"\"\"\n",
    "\n",
    "    if few_shot_prompt:\n",
    "        prompt = few_shot_prompt + '\\n'\n",
    "    else:\n",
    "        prompt = ''\n",
    "\n",
    "    prompt += 'Question: ' + question\n",
    "    prompt += '\\nBrainstormed Answers: '\n",
    "    for answer in brainstormed_answers + [most_probable_answer]:\n",
    "        prompt += answer.strip() + '\\n'\n",
    "    prompt += 'Possible answer: ' + most_probable_answer + '\\n'\n",
    "    if not hint:\n",
    "        prompt += 'Is the possible answer:\\n'\n",
    "        prompt += 'A) True\\n'\n",
    "        prompt += 'B) False\\n'\n",
    "        prompt += 'The possible answer is:'\n",
    "    else:\n",
    "        prompt += 'Do the brainstormed answers match the possible answer? Respond with A if they do, if they do not respond with B. Answer:'\n",
    "\n",
    "    log_prob = model.get_p_true(prompt)\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "281ed880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference(example):\n",
    "    if 'answers' not in example:\n",
    "        example = example['reference']\n",
    "    answers = example['answers']\n",
    "    answer_starts = answers.get('answer_start', [])\n",
    "    reference = {'answers': {'answer_start': answer_starts, 'text': answers['text']}, 'id': example['id']}\n",
    "    return reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64629b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "WARNING:root:Generation interrupted by max_token limit.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "p_true_indices = random.sample(answerable_indices, p_true_num_fewshot)\n",
    "remaining_answerable = list(set(remaining_answerable) - set(p_true_indices))\n",
    "p_true_few_shot_prompt, p_true_responses, len_p_true = construct_few_shot_prompt(\n",
    "            model=model, dataset=train_dataset, indices=p_true_indices,\n",
    "            prompt=prompt, brief=BRIEF,\n",
    "            brief_always=False,\n",
    "            make_prompt=make_prompt, num_generations=num_generations,\n",
    "            metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c122681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question: What information technology company was founded by Sergey Brin and Larry Page?\\nBrainstormed Answers: google \\ngoogle \\ngoogle \\ngoogle \\ngoogle \\ngoogle \\nPossible answer: google\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A\\nQuestion: Which musical term means ‘very loud’?\\nBrainstormed Answers: ppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp \\npppppppppp \\nppp \\nppppp \\npppppp \\nFortissimo (ff) \\nPossible answer: ppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: B\\nQuestion: Which 'superhero's' alter ego is 'Peter Parker'?\\nBrainstormed Answers: Spider-Man \\nSpidey \\nspider-man \\nSpider-Man \\nspider-man \\nSpider-man \\nPossible answer: Spider-Man\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: B\\nQuestion: What drink consists of vodka, Triple Sec, cranberry juice, and lime juice?\\nBrainstormed Answers: Cosmopolitan Cocktail \\nCosmopolitan Cocktail \\ncosmopolitan cocktail \\ncosmopolitan cocktail \\nCosmopolitan Cocktail \\ncosmopolitan cocktail \\nPossible answer: Cosmopolitan Cocktail\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_true_few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "936d5518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4046: {'responses': ['google',\n",
       "   'google',\n",
       "   'google',\n",
       "   'google',\n",
       "   'google',\n",
       "   'google'],\n",
       "  'most_likely_response': 'google',\n",
       "  'is_correct': 1.0},\n",
       " 7839: {'responses': ['ppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp',\n",
       "   'pppppppppp',\n",
       "   'ppp',\n",
       "   'ppppp',\n",
       "   'pppppp',\n",
       "   'Fortissimo (ff)'],\n",
       "  'most_likely_response': 'ppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp',\n",
       "  'is_correct': 0.0},\n",
       " 3247: {'responses': ['Spider-Man',\n",
       "   'Spidey',\n",
       "   'spider-man',\n",
       "   'Spider-Man',\n",
       "   'spider-man',\n",
       "   'Spider-man'],\n",
       "  'most_likely_response': 'Spider-Man',\n",
       "  'is_correct': 0.0},\n",
       " 10360: {'responses': ['Cosmopolitan Cocktail',\n",
       "   'Cosmopolitan Cocktail',\n",
       "   'cosmopolitan cocktail',\n",
       "   'cosmopolitan cocktail',\n",
       "   'Cosmopolitan Cocktail',\n",
       "   'cosmopolitan cocktail'],\n",
       "  'most_likely_response': 'Cosmopolitan Cocktail',\n",
       "  'is_correct': 1.0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_true_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690c381",
   "metadata": {},
   "source": [
    "Start generation answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df1596c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_training_set_generations = True\n",
    "temperature = 0.1\n",
    "get_training_set_generations_most_likely_only = True\n",
    "num_samples = 2\n",
    "compute_p_true = True\n",
    "p_true_hint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "367a2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      " 50%|█████     | 1/2 [00:00<00:00,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.38it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "for dataset_split in ['train', 'validation']:\n",
    "\n",
    "        # This will store all input data and model predictions.\n",
    "        accuracies, generations, results_dict, p_trues = [], {}, {}, []\n",
    "\n",
    "        if dataset_split == 'train':\n",
    "            if not get_training_set_generations:\n",
    "                logging.info('Skip training data.')\n",
    "                continue\n",
    "            dataset = train_dataset\n",
    "            possible_indices = list(set(remaining_answerable) | set(unanswerable_indices))\n",
    "\n",
    "        else:\n",
    "            dataset = validation_dataset\n",
    "            possible_indices = range(0, len(dataset))\n",
    "\n",
    "        # Evaluate over random subset of the datasets.\n",
    "        indices = random.sample(possible_indices, min(num_samples, len(dataset)))\n",
    "\n",
    "        #if args.num_samples > len(dataset):\n",
    "        #    logging.warning('Not enough samples in dataset. Using all %d samples.', len(dataset))\n",
    "\n",
    "        it = 0\n",
    "        for index in tqdm(indices):\n",
    "            if (it + 1 % 10) == 0:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            it += 1\n",
    "\n",
    "            # Grab example at index.\n",
    "            example = dataset[index]\n",
    "            question, context = example[\"question\"], example['context']\n",
    "            generations[example['id']] = {'question': question, 'context': context}\n",
    "            correct_answer = example['answers']['text']\n",
    "\n",
    "            current_input = make_prompt(\n",
    "                context, question, None, BRIEF, True)\n",
    "            local_prompt = prompt + current_input\n",
    "\n",
    "            logging.info('Current input: '.ljust(15) + current_input)\n",
    "\n",
    "            full_responses = []\n",
    "\n",
    "            # We sample one low temperature answer on which we will compute the\n",
    "            # accuracy and args.num_generation high temperature answers which will\n",
    "            # be used to estimate the entropy variants.\n",
    "\n",
    "            if dataset_split == 'train' and get_training_set_generations_most_likely_only:\n",
    "                num_generations = 1\n",
    "            else:\n",
    "                num_generations = num_generations + 1 # be careful num_generations is the same as above\n",
    "            \n",
    "            for i in range(num_generations):\n",
    "\n",
    "                # Temperature for first generation is always `0.1`.\n",
    "                temperature = 0.1 if i == 0 else temperature\n",
    "\n",
    "                predicted_answer, token_log_likelihoods, embedding = model.predict(\n",
    "                    local_prompt, temperature)\n",
    "                embedding = embedding.cpu() if embedding is not None else None\n",
    "\n",
    "                # Only compute accuracy if question is answerable.\n",
    "                compute_acc = True or (i == 0)\n",
    "                if correct_answer and compute_acc:\n",
    "                    acc = metric(predicted_answer, example, model)\n",
    "                else:\n",
    "                    acc = 0.0  # pylint: disable=invalid-name\n",
    "\n",
    "                if i == 0:\n",
    "                    logging.info('Iteration ' + str(it) + ':  ' + 80*'#')\n",
    "                    #if args.use_context:\n",
    "                    #    logging.info('context: '.ljust(15) + str(context))\n",
    "                    logging.info('question: '.ljust(15) + question)\n",
    "                    logging.info('low-t prediction: '.ljust(15) + predicted_answer)\n",
    "                    logging.info('correct answer: '.ljust(15) + str(correct_answer))\n",
    "                    logging.info('accuracy: '.ljust(15) + str(acc))\n",
    "\n",
    "                    accuracies.append(acc)\n",
    "\n",
    "                    most_likely_answer_dict = {\n",
    "                        'response': predicted_answer,\n",
    "                        'token_log_likelihoods': token_log_likelihoods,\n",
    "                        'embedding': embedding,\n",
    "                        'accuracy': acc}\n",
    "                    generations[example['id']].update({\n",
    "                        'most_likely_answer': most_likely_answer_dict,\n",
    "                        'reference': get_reference(example)})\n",
    "                    \n",
    "                else:\n",
    "                    logging.info('high-t prediction '.ljust(15) + str(i) + ' : ' + predicted_answer)\n",
    "                    # Aggregate predictions over num_generations.\n",
    "                    full_responses.append(\n",
    "                        (predicted_answer, token_log_likelihoods, embedding, acc))\n",
    "\n",
    "            # Append all predictions for this example to `generations`.\n",
    "            generations[example['id']]['responses'] = full_responses\n",
    "\n",
    "            if compute_p_true and dataset_split == 'validation':\n",
    "                # Already compute p_true here. Avoid cost of generations in compute_uncertainty script.\n",
    "                p_true = calculate_p_true(\n",
    "                    model, question, most_likely_answer_dict['response'],\n",
    "                    [r[0] for r in full_responses], p_true_few_shot_prompt,\n",
    "                    hint=p_true_hint)\n",
    "                p_trues.append(p_true)\n",
    "                logging.info('p_true: %s', p_true)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c3b522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23023097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tc_2306--184/184_37362.txt#0_0': {'question': 'Who did Jack Ruby shoot in November 1963?', 'context': \"[DOC] [TLE] Jack Ruby - Facts & Summary - HISTORY.comJack Ruby - Facts & Summary - HISTORY.com [PAR] Google [PAR] Who Was Jack Ruby? [PAR] Jacob Rubenstein, later known as Jack Ruby, was born in Chicago in 1911, the son of Polish immigrants. Official records list conflicting dates for Ruby’s birth; however, he used March 25, 1911, on his driver’s license. [PAR] Did You Know? [PAR] In 2009, the gray fedora worn by Jack Ruby when he shot Lee Harvey Oswald sold for $53,775 at a Dallas auction. The shackles Ruby wore when dying at Dallas’ Parkland Memorial Hospital sold for over $11,000, while an X-ray of Ruby's head went for more than $700. [PAR] Ruby, one of eight siblings, had a troubled childhood in Chicago and spent time in foster care. He never graduated from high school and spent years working odd jobs, including as a door-to-door salesman and ticket scalper. During World War II , Ruby served in the Army Air Forces, working as an aircraft mechanic at U.S. bases. By the late 1940s, he had moved to Dallas, where he became a small-time operator in the world of nightclubs and gambling. He also racked up a series of arrests for minor offenses. \", 'most_likely_answer': {'response': 'Lee Harvey Oswald', 'token_log_likelihoods': [0.0, 0.0, 0.0], 'embedding': tensor([[-1.7793,  2.3730, -1.3477,  ..., -0.0702,  2.0234,  0.1315]],\n",
      "       dtype=torch.float16), 'accuracy': 1.0}, 'reference': {'answers': {'answer_start': [440], 'text': ['lee harvey oswald']}, 'id': 'tc_2306--184/184_37362.txt#0_0'}, 'responses': [('Lee Harvey Oswald', [0.0, 0.0, 0.0], tensor([[-1.7793,  2.3730, -1.3477,  ..., -0.0702,  2.0234,  0.1315]],\n",
      "       dtype=torch.float16), 1.0)]}, 'odql_12252--88/88_2828858.txt#0_1': {'question': 'Which book of the Old Testament explains how the festival of Purim came to be celebrated by the Jews?', 'context': 'The massacre had been plotted by the king’s chief minister, Haman, and the date decided by casting lots (purim). Instead, Haman was hanged on the gallows he built for Mordecai; and on the day planned for their annihilation, the Jews destroyed their enemies. According to the Book of Esther, the feast of Purim was established to celebrate that day, but this explanation is surely legendary. There is nothing close to a consensus , however, as to what historical event provided the basis for the story. The book may have been composed as late as the first half of the 2nd century bc, though the origin of the Purim festival could date to the Babylonian exile (6th century bc).[DOC] [TLE] Purim symbols - timeanddate.comPurim [PAR] Home \\xa0 Calendar \\xa0 Holidays \\xa0 Purim [PAR] Purim [PAR] Purim is a Jewish observance on the 14th day of the month of Adar in the Jewish calendar, which is in February or March in the Gregorian calendar. It commemorates a time when Jewish people were saved from death in the Persian empire around the fourth century BCE, according to the Book of Esther. [PAR] People give food to those in need as part of their duties during Purim. [PAR] People give food to those in need as part of their duties during Purim. [PAR] ©iStockphoto.com/Howard Sandler [PAR] What Do People Do? [PAR] Many Jewish communities start the celebrations around sunset on the 13th day of Adar, while others observe Purim on the 15th day of Adar. ', 'most_likely_answer': {'response': 'Book of Esther', 'token_log_likelihoods': [0.0, 0.0, 0.0], 'embedding': tensor([[ 2.2090,  0.0944,  0.5234,  ..., -1.7139, -0.7568, -0.8916]],\n",
      "       dtype=torch.float16), 'accuracy': 1.0}, 'reference': {'answers': {'answer_start': [1072], 'text': ['esther']}, 'id': 'odql_12252--88/88_2828858.txt#0_1'}, 'responses': [('Book of Esther', [0.0, 0.0, 0.0], tensor([[ 2.2090,  0.0944,  0.5234,  ..., -1.7139, -0.7568, -0.8916]],\n",
      "       dtype=torch.float16), 1.0), ('Book of Esther', [0.0, 0.0, 0.0], tensor([[ 2.2090,  0.0944,  0.5234,  ..., -1.7139, -0.7568, -0.8916]],\n",
      "       dtype=torch.float16), 1.0)]}} validation_generations.pkl\n"
     ]
    }
   ],
   "source": [
    "print(generations,f'{dataset_split}_generations.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84c7158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall validation split accuracy: 1.0\n",
      "{'uncertainty_measures': {'p_false': [1.4056010842323303, 1.1104959100484848], 'p_false_fixed': [np.float64(0.33342397791988787), np.float64(0.10461000771634721)]}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Log overall accuracy.\n",
    "accuracy = np.mean(accuracies)\n",
    "print(f\"Overall {dataset_split} split accuracy: {accuracy}\")\n",
    "if dataset_split == 'validation':\n",
    "    results_dict['uncertainty_measures'] = {'p_false':  [1 - p for p in p_trues],'p_false_fixed':  [1 - np.exp(p) for p in p_trues],}\n",
    "\n",
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b3131",
   "metadata": {},
   "source": [
    "# uncertainty measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431f425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98b507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb2f82d8",
   "metadata": {},
   "source": [
    "# Annexe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb; print(bnb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59921ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; print(torch.version.cuda, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0  # optionnel, peut stabiliser la mémoire\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",       # répartit sur GPU(s)\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970662fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e362a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78197668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ff7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU libre  (GB): 11.492919921875\n",
      "GPU total  (GB): 14.56805419921875\n"
     ]
    }
   ],
   "source": [
    "free, total = torch.cuda.mem_get_info()\n",
    "print(\"GPU libre  (GB):\", free / 1024**3)\n",
    "print(\"GPU total  (GB):\", total / 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b2914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10016be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6efe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible :\", torch.cuda.is_available())\n",
    "print(\"Nombre de GPU :\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    idx = torch.cuda.current_device()\n",
    "    print(\"GPU actuel :\", torch.cuda.get_device_name(idx))\n",
    "    print(\"VRAM totale (GB) :\", torch.cuda.get_device_properties(idx).total_memory / 1024**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall bitsandbytes -y\n",
    "%pip install bitsandbytes-cuda128\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
